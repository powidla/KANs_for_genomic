{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "from pathlib import Path\n",
        "from typing import ClassVar, Iterator, Sequence\n",
        "import json\n",
        "\n",
        "# import mmh3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler"
      ],
      "metadata": {
        "id": "iWeDk-z75g4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KAN Linear"
      ],
      "metadata": {
        "id": "vI0xJvw1SnDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KANLinear(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        grid_size=20,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        enable_standalone_scale_spline=True,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KANLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = (\n",
        "            (\n",
        "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
        "                + grid_range[0]\n",
        "            )\n",
        "            .expand(in_features, -1)\n",
        "            .contiguous()\n",
        "        )\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = torch.nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
        "        )\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = torch.nn.Parameter(\n",
        "                torch.Tensor(out_features, in_features)\n",
        "            )\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = (\n",
        "                (\n",
        "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
        "                    - 1 / 2\n",
        "                )\n",
        "                * self.scale_noise\n",
        "                / self.grid_size\n",
        "            )\n",
        "            self.spline_weight.data.copy_(\n",
        "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
        "                * self.curve2coeff(\n",
        "                    self.grid.T[self.spline_order : -self.spline_order],\n",
        "                    noise,\n",
        "                )\n",
        "            )\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
        "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the B-spline bases for the given input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "\n",
        "        grid: torch.Tensor = (\n",
        "            self.grid\n",
        "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = (\n",
        "                (x - grid[:, : -(k + 1)])\n",
        "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
        "                * bases[:, :, :-1]\n",
        "            ) + (\n",
        "                (grid[:, k + 1 :] - x)\n",
        "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
        "                * bases[:, :, 1:]\n",
        "            )\n",
        "\n",
        "        assert bases.size() == (\n",
        "            x.size(0),\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the coefficients of the curve that interpolates the given points.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
        "\n",
        "        A = self.b_splines(x).transpose(\n",
        "            0, 1\n",
        "        )  # (in_features, batch_size, grid_size + spline_order)\n",
        "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
        "        solution = torch.linalg.lstsq(\n",
        "            A, B\n",
        "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
        "        result = solution.permute(\n",
        "            2, 0, 1\n",
        "        )  # (out_features, in_features, grid_size + spline_order)\n",
        "\n",
        "        assert result.size() == (\n",
        "            self.out_features,\n",
        "            self.in_features,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return result.contiguous()\n",
        "\n",
        "    @property\n",
        "    def scaled_spline_weight(self):\n",
        "        return self.spline_weight * (\n",
        "            self.spline_scaler.unsqueeze(-1)\n",
        "            if self.enable_standalone_scale_spline\n",
        "            else 1.0\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.size(-1) == self.in_features\n",
        "        original_shape = x.shape\n",
        "        x = x.reshape(-1, self.in_features)\n",
        "\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(\n",
        "            self.b_splines(x).view(x.size(0), -1),\n",
        "            self.scaled_spline_weight.view(self.out_features, -1),\n",
        "        )\n",
        "        output = base_output + spline_output\n",
        "\n",
        "        output = output.reshape(*original_shape[:-1], self.out_features)\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        batch = x.size(0)\n",
        "\n",
        "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
        "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
        "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
        "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
        "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
        "        unreduced_spline_output = unreduced_spline_output.permute(\n",
        "            1, 0, 2\n",
        "        )  # (batch, in, out)\n",
        "\n",
        "        # sort each channel individually to collect data distribution\n",
        "        x_sorted = torch.sort(x, dim=0)[0]\n",
        "        grid_adaptive = x_sorted[\n",
        "            torch.linspace(\n",
        "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
        "        grid_uniform = (\n",
        "            torch.arange(\n",
        "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
        "            ).unsqueeze(1)\n",
        "            * uniform_step\n",
        "            + x_sorted[0]\n",
        "            - margin\n",
        "        )\n",
        "\n",
        "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
        "        grid = torch.concatenate(\n",
        "            [\n",
        "                grid[:1]\n",
        "                - uniform_step\n",
        "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
        "                grid,\n",
        "                grid[-1:]\n",
        "                + uniform_step\n",
        "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        self.grid.copy_(grid.T)\n",
        "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        \"\"\"\n",
        "        Compute the regularization loss.\n",
        "\n",
        "        This is a dumb simulation of the original L1 regularization as stated in the\n",
        "        paper, since the original one requires computing absolutes and entropy from the\n",
        "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
        "        behind the F.linear function if we want an memory efficient implementation.\n",
        "\n",
        "        The L1 regularization is now computed as mean absolute value of the spline\n",
        "        weights. The authors implementation also includes this term in addition to the\n",
        "        sample-based regularization.\n",
        "        \"\"\"\n",
        "        l1_fake = self.spline_weight.abs().mean(-1)\n",
        "        regularization_loss_activation = l1_fake.sum()\n",
        "        p = l1_fake / regularization_loss_activation\n",
        "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
        "        return (\n",
        "            regularize_activation * regularization_loss_activation\n",
        "            + regularize_entropy * regularization_loss_entropy\n",
        "        )\n",
        "\n",
        "\n",
        "class KAN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers_hidden,\n",
        "        grid_size=20,\n",
        "        spline_order=3,\n",
        "        scale_noise=0.1,\n",
        "        scale_base=1.0,\n",
        "        scale_spline=1.0,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "        grid_eps=0.02,\n",
        "        grid_range=[-1, 1],\n",
        "    ):\n",
        "        super(KAN, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
        "            self.layers.append(\n",
        "                KANLinear(\n",
        "                    in_features,\n",
        "                    out_features,\n",
        "                    grid_size=grid_size,\n",
        "                    spline_order=spline_order,\n",
        "                    scale_noise=scale_noise,\n",
        "                    scale_base=scale_base,\n",
        "                    scale_spline=scale_spline,\n",
        "                    base_activation=base_activation,\n",
        "                    grid_eps=grid_eps,\n",
        "                    grid_range=grid_range,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, update_grid=True):\n",
        "        for layer in self.layers:\n",
        "            if update_grid:\n",
        "                layer.update_grid(x)\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        return sum(\n",
        "            layer.regularization_loss(regularize_activation, regularize_entropy)\n",
        "            for layer in self.layers\n",
        "        )"
      ],
      "metadata": {
        "id": "pTu2V80lvv6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KAN Conv"
      ],
      "metadata": {
        "id": "lTlxIJIGPr7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Util\n",
        "def add_padding_1d(array: np.ndarray, padding: int) -> np.ndarray:\n",
        "    \"\"\"Adds padding to a 1D array.\"\"\"\n",
        "    n = array.shape[0]\n",
        "    padded_array = np.zeros(n + 2 * padding)\n",
        "    padded_array[padding: n + padding] = array\n",
        "    return padded_array\n",
        "\n",
        "\n",
        "def calc_out_dims_1d(array, kernel_size, stride, dilation, padding):\n",
        "    \"\"\"Calculate output dimensions for 1D convolution.\"\"\"\n",
        "    batch_size, n_channels, n = matrix.shape\n",
        "    out_size = np.floor((n + 2 * padding - kernel_size - (kernel_size - 1) * (dilation - 1)) / stride).astype(int) + 1\n",
        "    return out_size, batch_size, n_channels\n",
        "\n",
        "\n",
        "def multiple_convs_kan_conv1d(array,\n",
        "                               kernels,\n",
        "                               kernel_size,\n",
        "                               out_channels,\n",
        "                               stride=1,\n",
        "                               dilation=1,\n",
        "                               padding=0,\n",
        "                               device=\"cuda\") -> torch.Tensor:\n",
        "    \"\"\"Performs a 1D convolution with multiple kernels on the input array using specified stride, dilation, and padding.\n",
        "\n",
        "    Args:\n",
        "        array (torch.Tensor): 1D tensor of shape (batch_size, channels, length).\n",
        "        kernels (list): List of kernel functions to be applied.\n",
        "        kernel_size (int): Size of the 1D kernel.\n",
        "        out_channels (int): Number of output channels.\n",
        "        stride (int): Stride along the length of the array. Default is 1.\n",
        "        dilation (int): Dilation rate along the length of the array. Default is 1.\n",
        "        padding (int): Number of elements to pad on each side. Default is 0.\n",
        "        device (str): Device to perform calculations on. Default is \"cuda\".\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Feature map after convolution with shape (batch_size, out_channels, length_out).\n",
        "    \"\"\"\n",
        "    length_out, batch_size = calc_out_dims_1d(array, kernel_size, stride, dilation, padding)\n",
        "    n_convs = len(kernels)\n",
        "\n",
        "    array_out = torch.zeros((batch_size, out_channels, length_out)).to(device)\n",
        "\n",
        "    array = F.pad(array, (padding, padding), mode='constant', value=0)\n",
        "    conv_groups = array.unfold(2, kernel_size, stride)\n",
        "    conv_groups = conv_groups.contiguous()\n",
        "\n",
        "    kern_per_out = len(kernels) // out_channels\n",
        "\n",
        "    for c_out in range(out_channels):\n",
        "        out_channel_accum = torch.zeros((batch_size, length_out), device=device)\n",
        "\n",
        "        for k_idx in range(kern_per_out):\n",
        "            kernel = kernels[c_out * kern_per_out + k_idx]\n",
        "            conv_result = kernel(conv_groups.view(-1, 1, kernel_size))\n",
        "            out_channel_accum += conv_result.view(batch_size, length_out)\n",
        "\n",
        "        array_out[:, c_out, :] = out_channel_accum\n",
        "\n",
        "    return array_out"
      ],
      "metadata": {
        "id": "4UbM_j1ePw-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kan_conv1d(matrix: torch.Tensor,\n",
        "               kernel,\n",
        "               kernel_size: int,\n",
        "               stride: int = 1,\n",
        "               dilation: int = 1,\n",
        "               padding: int = 0,\n",
        "               device: str = \"cpu\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs a 1D convolution with the given kernel over a 1D matrix using the defined stride, dilation, and padding.\n",
        "\n",
        "    Args:\n",
        "        matrix (torch.Tensor): 3D tensor (batch_size, channels, width) to be convolved.\n",
        "        kernel (function): Kernel function to apply on the 1D patches of the matrix.\n",
        "        kernel_size (int): Size of the kernel (assumed to be square).\n",
        "        stride (int, optional): Stride along the width axis. Defaults to 1.\n",
        "        dilation (int, optional): Dilation along the width axis. Defaults to 1.\n",
        "        padding (int, optional): Padding along the width axis. Defaults to 0.\n",
        "        device (str): Device to perform the operation on (e.g., \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 1D Feature map after convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, n_channels, width_in = matrix.shape\n",
        "    width_out = ((width_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride) + 1\n",
        "    matrix_out = torch.zeros((batch_size, n_channels, width_out), device=device)\n",
        "\n",
        "    matrix_padded = torch.nn.functional.pad(matrix, (padding, padding))\n",
        "\n",
        "    for i in range(width_out):\n",
        "\n",
        "        start = i * stride\n",
        "        end = start + kernel_size * dilation\n",
        "        patch = matrix_padded[:, :, start:end:dilation]\n",
        "\n",
        "        matrix_out[:, :, i] = kernel.forward(patch).squeeze(-1)\n",
        "\n",
        "    return matrix_out"
      ],
      "metadata": {
        "id": "YMJ86gElPxBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KAN_Convolutional_Layer_1D(torch.nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=0, dilation=1, device=\"cuda\"):\n",
        "        super(KAN_Convolutional_Layer_1D, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.device = device\n",
        "        self.convs = torch.nn.ModuleList([KAN_Convolution_1D(kernel_size, stride, padding, dilation, device) for _ in range(in_channels * out_channels)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return torch.cat([conv(x[:, i, :].unsqueeze(1)) for i, conv in enumerate(self.convs)], dim=1)"
      ],
      "metadata": {
        "id": "k7vXkJNbPxDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KAN_Convolutional_Layer_1D(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int = 1,\n",
        "            out_channels: int = 1,\n",
        "            kernel_size: int = 2,\n",
        "            stride: int = 1,\n",
        "            padding: int = 0,\n",
        "            dilation: int = 1,\n",
        "            grid_size: int = 5,\n",
        "            spline_order: int = 3,\n",
        "            scale_noise: float = 0.1,\n",
        "            scale_base: float = 1.0,\n",
        "            scale_spline: float = 1.0,\n",
        "            base_activation=torch.nn.SiLU,\n",
        "            grid_eps: float = 0.02,\n",
        "            grid_range: tuple = [-1, 1],\n",
        "            device: str = \"cpu\"\n",
        "        ):\n",
        "        super(KAN_Convolutional_Layer_1D, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.in_channels = in_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for _ in range(in_channels * out_channels):\n",
        "            self.convs.append(\n",
        "                KAN_Convolution_1D(\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=stride,\n",
        "                    padding=padding,\n",
        "                    dilation=dilation,\n",
        "                    grid_size=grid_size,\n",
        "                    spline_order=spline_order,\n",
        "                    scale_noise=scale_noise,\n",
        "                    scale_base=scale_base,\n",
        "                    scale_spline=scale_spline,\n",
        "                    base_activation=base_activation,\n",
        "                    grid_eps=grid_eps,\n",
        "                    grid_range=grid_range,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        batch_size, in_channels, length = x.shape\n",
        "        output_length = (length + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n",
        "        output = torch.zeros((batch_size, self.out_channels, output_length), device=x.device)\n",
        "\n",
        "\n",
        "        for i in range(self.out_channels):\n",
        "            output_accum = torch.zeros((batch_size, output_length), device=x.device)\n",
        "            for j in range(self.in_channels):\n",
        "                kernel_idx = i * self.in_channels + j\n",
        "                conv_result = self.convs[kernel_idx].forward(x[:, j, :].unsqueeze(1))\n",
        "                output_accum += conv_result.squeeze(1)  # Squeeze\n",
        "            output[:, i, :] = output_accum  # A to output channel\n",
        "\n",
        "        return output\n",
        "\n",
        "class KAN_Convolution_1D(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            kernel_size: int = 2,\n",
        "            stride: int = 1,\n",
        "            padding: int = 0,\n",
        "            dilation: int = 1,\n",
        "            grid_size: int = 50,\n",
        "            spline_order: int = 3,\n",
        "            scale_noise: float = 0.1,\n",
        "            scale_base: float = 1.0,\n",
        "            scale_spline: float = 1.0,\n",
        "            base_activation=torch.nn.SiLU,\n",
        "            grid_eps: float = 0.02,\n",
        "            grid_range: tuple = [-1, 1]\n",
        "        ):\n",
        "        super(KAN_Convolution_1D, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.conv = KANLinear(\n",
        "            in_features = kernel_size,\n",
        "            out_features = 1,\n",
        "            grid_size=grid_size,\n",
        "            spline_order=spline_order,\n",
        "            scale_noise=scale_noise,\n",
        "            scale_base=scale_base,\n",
        "            scale_spline=scale_spline,\n",
        "            base_activation=base_activation,\n",
        "            grid_eps=grid_eps,\n",
        "            grid_range=grid_range\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        self.device = x.device\n",
        "        return kan_conv1d(x, self.conv, self.kernel_size,self.stride, self.dilation, self.padding, self.device)"
      ],
      "metadata": {
        "id": "wNgoVQDoP4nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with changed mapper"
      ],
      "metadata": {
        "id": "4aF6i9lzP9Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SeqNN(nn.Module):\n",
        "#     \"\"\"\n",
        "#     SeqNN neural network for binary classification with modified mapper using KAN_Convolutional_Layer_1D.\n",
        "#     \"\"\"\n",
        "#     def __init__(self,\n",
        "#                  seqsize,\n",
        "#                  use_single_channel,\n",
        "#                  use_reverse_channel,\n",
        "#                  use_multisubstate_channel,\n",
        "#                  block_sizes=[256, 256, 128, 128, 64, 64, 32, 32],\n",
        "#                  ks=5,\n",
        "#                  resize_factor=4,\n",
        "#                  activation=nn.SiLU,\n",
        "#                  filter_per_group=2,\n",
        "#                  se_reduction=4,\n",
        "#                  final_ch=1,\n",
        "#                  bn_momentum=0.1):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.block_sizes = block_sizes\n",
        "#         self.resize_factor = resize_factor\n",
        "#         self.se_reduction = se_reduction\n",
        "#         self.seqsize = seqsize\n",
        "#         self.use_single_channel = use_single_channel\n",
        "#         self.use_reverse_channel = use_reverse_channel\n",
        "#         self.use_multisubstate_channel = use_multisubstate_channel\n",
        "#         self.final_ch = final_ch\n",
        "#         self.bn_momentum = bn_momentum\n",
        "\n",
        "#         seqextblocks = OrderedDict()\n",
        "#         in_channels_first_block = 4  # 4 channels for A, T, C, G\n",
        "#         if self.use_single_channel:\n",
        "#             in_channels_first_block += 1\n",
        "#         if self.use_reverse_channel:\n",
        "#             in_channels_first_block += 1\n",
        "#         if self.use_multisubstate_channel:\n",
        "#             in_channels_first_block += 1\n",
        "\n",
        "#         # First layer\n",
        "#         block = nn.Sequential(\n",
        "#             nn.Conv1d(\n",
        "#                 in_channels=in_channels_first_block,\n",
        "#                 out_channels=block_sizes[0],\n",
        "#                 kernel_size=ks,\n",
        "#                 padding='same',\n",
        "#                 bias=False\n",
        "#             ),\n",
        "#             nn.BatchNorm1d(block_sizes[0], momentum=self.bn_momentum),\n",
        "#             activation()\n",
        "#         )\n",
        "#         seqextblocks['blc0'] = block\n",
        "\n",
        "#         # Building remaining blocks\n",
        "#         for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
        "#             block = nn.Sequential(\n",
        "#                 nn.Conv1d(prev_sz, sz * self.resize_factor, kernel_size=1, padding='same', bias=False),\n",
        "#                 nn.BatchNorm1d(sz * self.resize_factor, momentum=self.bn_momentum),\n",
        "#                 activation(),\n",
        "\n",
        "#                 nn.Conv1d(sz * self.resize_factor, sz * self.resize_factor, kernel_size=ks,\n",
        "#                           groups=sz * self.resize_factor // filter_per_group, padding='same', bias=False),\n",
        "#                 nn.BatchNorm1d(sz * self.resize_factor, momentum=self.bn_momentum),\n",
        "#                 activation(),\n",
        "\n",
        "#                 SELayer(prev_sz, sz * self.resize_factor, reduction=self.se_reduction),\n",
        "\n",
        "#                 nn.Conv1d(sz * self.resize_factor, prev_sz, kernel_size=1, padding='same', bias=False),\n",
        "#                 nn.BatchNorm1d(prev_sz, momentum=self.bn_momentum),\n",
        "#                 activation(),\n",
        "#             )\n",
        "#             seqextblocks[f'inv_res_blc{ind}'] = block\n",
        "\n",
        "#             resize_block = nn.Sequential(\n",
        "#                 nn.Conv1d(2 * prev_sz, sz, kernel_size=ks, padding='same', bias=False),\n",
        "#                 nn.BatchNorm1d(sz, momentum=self.bn_momentum),\n",
        "#                 activation()\n",
        "#             )\n",
        "#             seqextblocks[f'resize_blc{ind}'] = resize_block\n",
        "\n",
        "#         self.seqextractor = nn.ModuleDict(seqextblocks)\n",
        "\n",
        "#         self.mapper = nn.Sequential(\n",
        "#             KAN_Convolutional_Layer_1D(\n",
        "#                 in_channels=block_sizes[-1],\n",
        "#                 out_channels=self.final_ch,\n",
        "#                 kernel_size=1,\n",
        "#                 padding=0,\n",
        "#                 device=\"cuda\"\n",
        "#             ),\n",
        "#             activation()\n",
        "#         )\n",
        "\n",
        "#         self.register_buffer('bins', torch.arange(start=0, end=self.final_ch, step=1, requires_grad=False))\n",
        "\n",
        "#     def feature_extractor(self, x):\n",
        "#         x = self.seqextractor['blc0'](x)\n",
        "#         for i in range(len(self.block_sizes) - 1):\n",
        "#             x = torch.cat([x, self.seqextractor[f'inv_res_blc{i}'](x)], dim=1)\n",
        "#             x = self.seqextractor[f'resize_blc{i}'](x)\n",
        "#         return x\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         f = self.feature_extractor(x)\n",
        "#         x = self.mapper(f)\n",
        "#         x = F.adaptive_avg_pool1d(x, 1)\n",
        "#         x = x.squeeze(2)\n",
        "#         prob = torch.sigmoid(x).squeeze(1)\n",
        "\n",
        "#         return prob"
      ],
      "metadata": {
        "id": "kmaXRmAdP4q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_map = {\n",
        "    \"A\": [1, 0, 0, 0],\n",
        "    \"T\": [0, 1, 0, 0],\n",
        "    \"C\": [0, 0, 1, 0],\n",
        "    \"G\": [0, 0, 0, 1,]\n",
        "}"
      ],
      "metadata": {
        "id": "fIoKlbf84xHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "PzqUJs3V5Sib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "RMcNNTsP7F-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SeqNN(\n",
        "    seqsize=seq_length,\n",
        "    use_single_channel=False,\n",
        "    use_reverse_channel=False,\n",
        "    use_multisubstate_channel=False,\n",
        "    final_ch=1  # For binary classification\n",
        ")\n",
        "# criterion = nn.BCEWithLogitsLoss()  # Binary classification with logits\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.005)\n"
      ],
      "metadata": {
        "id": "UNgMwQp57y-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayyNG-mq8Ard",
        "outputId": "4434deaa-4457-4392-e4ba-fead9c6b255d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SeqNN(\n",
              "  (seqextractor): ModuleDict(\n",
              "    (blc0): Sequential(\n",
              "      (0): Conv1d(4, 256, kernel_size=(5,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "    )\n",
              "    (inv_res_blc0): Sequential(\n",
              "      (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "      (3): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=same, groups=512, bias=False)\n",
              "      (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): SiLU()\n",
              "      (6): SELayer(\n",
              "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
              "        (fc): Sequential(\n",
              "          (0): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (1): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (2): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (7): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): SiLU()\n",
              "    )\n",
              "    (resize_blc0): Sequential(\n",
              "      (0): Conv1d(512, 256, kernel_size=(5,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "    )\n",
              "    (inv_res_blc1): Sequential(\n",
              "      (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "      (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=same, groups=256, bias=False)\n",
              "      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): SiLU()\n",
              "      (6): SELayer(\n",
              "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
              "        (fc): Sequential(\n",
              "          (0): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (1): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (2): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (7): Conv1d(512, 256, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): SiLU()\n",
              "    )\n",
              "    (resize_blc1): Sequential(\n",
              "      (0): Conv1d(512, 128, kernel_size=(5,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "    )\n",
              "    (inv_res_blc2): Sequential(\n",
              "      (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "      (3): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=same, groups=256, bias=False)\n",
              "      (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): SiLU()\n",
              "      (6): SELayer(\n",
              "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
              "        (fc): Sequential(\n",
              "          (0): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (1): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (2): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (7): Conv1d(512, 128, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): SiLU()\n",
              "    )\n",
              "    (resize_blc2): Sequential(\n",
              "      (0): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "    )\n",
              "    (inv_res_blc3): Sequential(\n",
              "      (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "      (3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=same, groups=128, bias=False)\n",
              "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): SiLU()\n",
              "      (6): SELayer(\n",
              "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
              "        (fc): Sequential(\n",
              "          (0): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (1): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (2): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (7): Conv1d(256, 128, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): SiLU()\n",
              "    )\n",
              "    (resize_blc3): Sequential(\n",
              "      (0): Conv1d(256, 64, kernel_size=(5,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "    )\n",
              "    (inv_res_blc4): Sequential(\n",
              "      (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "      (3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=same, groups=128, bias=False)\n",
              "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): SiLU()\n",
              "      (6): SELayer(\n",
              "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
              "        (fc): Sequential(\n",
              "          (0): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (1): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (2): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (7): Conv1d(256, 64, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): SiLU()\n",
              "    )\n",
              "    (resize_blc4): Sequential(\n",
              "      (0): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "    )\n",
              "    (inv_res_blc5): Sequential(\n",
              "      (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "      (3): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=same, groups=64, bias=False)\n",
              "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): SiLU()\n",
              "      (6): SELayer(\n",
              "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
              "        (fc): Sequential(\n",
              "          (0): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (1): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (2): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (7): Conv1d(128, 64, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): SiLU()\n",
              "    )\n",
              "    (resize_blc5): Sequential(\n",
              "      (0): Conv1d(128, 32, kernel_size=(5,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "    )\n",
              "    (inv_res_blc6): Sequential(\n",
              "      (0): Conv1d(32, 128, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "      (3): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=same, groups=64, bias=False)\n",
              "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): SiLU()\n",
              "      (6): SELayer(\n",
              "        (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
              "        (fc): Sequential(\n",
              "          (0): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (1): KANLinear(\n",
              "            (base_activation): SiLU()\n",
              "          )\n",
              "          (2): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (7): Conv1d(128, 32, kernel_size=(1,), stride=(1,), padding=same, bias=False)\n",
              "      (8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): SiLU()\n",
              "    )\n",
              "    (resize_blc6): Sequential(\n",
              "      (0): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=same, bias=False)\n",
              "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): SiLU()\n",
              "    )\n",
              "  )\n",
              "  (mapper): Sequential(\n",
              "    (0): Conv1d(32, 1, kernel_size=(1,), stride=(1,), padding=same)\n",
              "    (1): SiLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB0Z02CDwoLd",
        "outputId": "13c83db9-a4b3-450a-9153-42affd21cd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 8891617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        probs = model(sequences)\n",
        "        loss = criterion(probs, labels.float())\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * sequences.size(0)\n",
        "        all_preds.extend(probs.detach().cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "\n",
        "    all_preds = [1 if p >= 0.5 else 0 for p in all_preds]\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, accuracy, f1, auc\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "            probs = model(sequences)\n",
        "            loss = criterion(probs, labels.float())\n",
        "            eval_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "            all_preds.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = eval_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "    all_preds = [1 if p >= 0.5 else 0 for p in all_preds]\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, accuracy, f1, auc\n"
      ],
      "metadata": {
        "id": "Gdzbmsx4738E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "demo_coding_vs_intergenomic_seqs grid = 10"
      ],
      "metadata": {
        "id": "WGyfaNlEp6mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc, train_f1, train_auc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_f1, val_auc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}\")\n",
        "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")"
      ],
      "metadata": {
        "id": "KfCrjtw-F8GD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6aacc5-706f-4ed6-d95a-72da0278b8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train - Loss: 0.5094, Acc: 0.8107, F1: 0.8071, AUC: 0.8107\n",
            "Val   - Loss: 0.4517, Acc: 0.8806, F1: 0.8775, AUC: 0.8806\n",
            "Epoch 2/10\n",
            "Train - Loss: 0.4517, Acc: 0.8632, F1: 0.8606, AUC: 0.8632\n",
            "Val   - Loss: 0.4257, Acc: 0.8844, F1: 0.8763, AUC: 0.8844\n",
            "Epoch 3/10\n",
            "Train - Loss: 0.4325, Acc: 0.8830, F1: 0.8816, AUC: 0.8830\n",
            "Val   - Loss: 0.4220, Acc: 0.8865, F1: 0.8921, AUC: 0.8865\n",
            "Epoch 4/10\n",
            "Train - Loss: 0.4252, Acc: 0.8895, F1: 0.8880, AUC: 0.8895\n",
            "Val   - Loss: 0.4152, Acc: 0.8962, F1: 0.8986, AUC: 0.8962\n",
            "Epoch 5/10\n",
            "Train - Loss: 0.4174, Acc: 0.8960, F1: 0.8949, AUC: 0.8960\n",
            "Val   - Loss: 0.4162, Acc: 0.8839, F1: 0.8910, AUC: 0.8839\n",
            "Epoch 6/10\n",
            "Train - Loss: 0.4138, Acc: 0.8985, F1: 0.8974, AUC: 0.8985\n",
            "Val   - Loss: 0.4031, Acc: 0.9072, F1: 0.9086, AUC: 0.9072\n",
            "Epoch 7/10\n",
            "Train - Loss: 0.4096, Acc: 0.9006, F1: 0.8992, AUC: 0.9006\n",
            "Val   - Loss: 0.4140, Acc: 0.9104, F1: 0.9082, AUC: 0.9104\n",
            "Epoch 8/10\n",
            "Train - Loss: 0.4050, Acc: 0.9047, F1: 0.9034, AUC: 0.9047\n",
            "Val   - Loss: 0.4151, Acc: 0.8986, F1: 0.8918, AUC: 0.8986\n",
            "Epoch 9/10\n",
            "Train - Loss: 0.4012, Acc: 0.9082, F1: 0.9072, AUC: 0.9082\n",
            "Val   - Loss: 0.3977, Acc: 0.9123, F1: 0.9106, AUC: 0.9123\n",
            "Epoch 10/10\n",
            "Train - Loss: 0.3986, Acc: 0.9085, F1: 0.9072, AUC: 0.9085\n",
            "Val   - Loss: 0.3967, Acc: 0.9086, F1: 0.9114, AUC: 0.9086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "demo_human_or_worm"
      ],
      "metadata": {
        "id": "Jt8sL48Lp8pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc, train_f1, train_auc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_f1, val_auc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}\")\n",
        "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKnWf12vdd3L",
        "outputId": "d6be25d7-1694-47a5-de89-de3058aee884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train - Loss: 0.4612, Acc: 0.8723, F1: 0.8693, AUC: 0.8723\n",
            "Val   - Loss: 0.3925, Acc: 0.9398, F1: 0.9389, AUC: 0.9400\n",
            "Epoch 2/10\n",
            "Train - Loss: 0.4037, Acc: 0.9218, F1: 0.9199, AUC: 0.9218\n",
            "Val   - Loss: 0.3896, Acc: 0.9479, F1: 0.9462, AUC: 0.9477\n",
            "Epoch 3/10\n",
            "Train - Loss: 0.3884, Acc: 0.9329, F1: 0.9313, AUC: 0.9329\n",
            "Val   - Loss: 0.3799, Acc: 0.9483, F1: 0.9462, AUC: 0.9479\n",
            "Epoch 4/10\n",
            "Train - Loss: 0.3791, Acc: 0.9396, F1: 0.9381, AUC: 0.9396\n",
            "Val   - Loss: 0.3710, Acc: 0.9517, F1: 0.9504, AUC: 0.9517\n",
            "Epoch 5/10\n",
            "Train - Loss: 0.3740, Acc: 0.9422, F1: 0.9406, AUC: 0.9421\n",
            "Val   - Loss: 0.3745, Acc: 0.9517, F1: 0.9493, AUC: 0.9511\n",
            "Epoch 6/10\n",
            "Train - Loss: 0.3704, Acc: 0.9457, F1: 0.9445, AUC: 0.9458\n",
            "Val   - Loss: 0.3744, Acc: 0.9514, F1: 0.9517, AUC: 0.9522\n",
            "Epoch 7/10\n",
            "Train - Loss: 0.3666, Acc: 0.9469, F1: 0.9454, AUC: 0.9468\n",
            "Val   - Loss: 0.3599, Acc: 0.9593, F1: 0.9583, AUC: 0.9593\n",
            "Epoch 8/10\n",
            "Train - Loss: 0.3628, Acc: 0.9495, F1: 0.9481, AUC: 0.9495\n",
            "Val   - Loss: 0.3596, Acc: 0.9579, F1: 0.9570, AUC: 0.9580\n",
            "Epoch 9/10\n",
            "Train - Loss: 0.3608, Acc: 0.9510, F1: 0.9497, AUC: 0.9510\n",
            "Val   - Loss: 0.3613, Acc: 0.9600, F1: 0.9592, AUC: 0.9602\n",
            "Epoch 10/10\n",
            "Train - Loss: 0.3598, Acc: 0.9520, F1: 0.9507, AUC: 0.9520\n",
            "Val   - Loss: 0.3739, Acc: 0.9478, F1: 0.9444, AUC: 0.9469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "human_enhancers_cohn"
      ],
      "metadata": {
        "id": "MUP60IHpC17L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc, train_f1, train_auc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_f1, val_auc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}\")\n",
        "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVjxRBTmCyLp",
        "outputId": "a2506f9d-4511-4717-c8c4-a9cd48f1142f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train - Loss: 0.6332, Acc: 0.6694, F1: 0.6726, AUC: 0.6694\n",
            "Val   - Loss: 0.6042, Acc: 0.7044, F1: 0.6776, AUC: 0.7044\n",
            "Epoch 2/10\n",
            "Train - Loss: 0.6230, Acc: 0.6822, F1: 0.6859, AUC: 0.6822\n",
            "Val   - Loss: 0.5933, Acc: 0.7191, F1: 0.7269, AUC: 0.7191\n",
            "Epoch 3/10\n",
            "Train - Loss: 0.6177, Acc: 0.6901, F1: 0.6946, AUC: 0.6901\n",
            "Val   - Loss: 0.5887, Acc: 0.7250, F1: 0.7142, AUC: 0.7250\n",
            "Epoch 4/10\n",
            "Train - Loss: 0.6122, Acc: 0.6968, F1: 0.7075, AUC: 0.6968\n",
            "Val   - Loss: 0.5945, Acc: 0.7166, F1: 0.7395, AUC: 0.7166\n",
            "Epoch 5/10\n",
            "Train - Loss: 0.6008, Acc: 0.7099, F1: 0.7134, AUC: 0.7099\n",
            "Val   - Loss: 0.6246, Acc: 0.6919, F1: 0.7425, AUC: 0.6919\n",
            "Epoch 6/10\n",
            "Train - Loss: 0.5975, Acc: 0.7149, F1: 0.7129, AUC: 0.7149\n",
            "Val   - Loss: 0.6043, Acc: 0.7117, F1: 0.7393, AUC: 0.7117\n",
            "Epoch 7/10\n",
            "Train - Loss: 0.5956, Acc: 0.7189, F1: 0.7159, AUC: 0.7189\n",
            "Val   - Loss: 0.5864, Acc: 0.7232, F1: 0.7049, AUC: 0.7232\n",
            "Epoch 8/10\n",
            "Train - Loss: 0.5948, Acc: 0.7210, F1: 0.7150, AUC: 0.7210\n",
            "Val   - Loss: 0.5905, Acc: 0.7165, F1: 0.7355, AUC: 0.7165\n",
            "Epoch 9/10\n",
            "Train - Loss: 0.5940, Acc: 0.7171, F1: 0.7109, AUC: 0.7171\n",
            "Val   - Loss: 0.5840, Acc: 0.7270, F1: 0.7283, AUC: 0.7270\n",
            "Epoch 10/10\n",
            "Train - Loss: 0.5924, Acc: 0.7206, F1: 0.7173, AUC: 0.7206\n",
            "Val   - Loss: 0.5905, Acc: 0.7163, F1: 0.7340, AUC: 0.7163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "human_nontata_promoters"
      ],
      "metadata": {
        "id": "vEszAavmiOz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc, train_f1, train_auc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_f1, val_auc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}\")\n",
        "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6Vtnf-DhIgh",
        "outputId": "9f7b49c2-42f2-43dd-fda1-6e6dc77353eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train - Loss: 0.4936, Acc: 0.7803, F1: 0.7843, AUC: 0.7847\n",
            "Val   - Loss: 0.4526, Acc: 0.8182, F1: 0.8114, AUC: 0.8278\n",
            "Epoch 2/10\n",
            "Train - Loss: 0.4569, Acc: 0.8109, F1: 0.8064, AUC: 0.8192\n",
            "Val   - Loss: 0.4372, Acc: 0.8327, F1: 0.8319, AUC: 0.8397\n",
            "Epoch 3/10\n",
            "Train - Loss: 0.4440, Acc: 0.8195, F1: 0.8157, AUC: 0.8278\n",
            "Val   - Loss: 0.4284, Acc: 0.8294, F1: 0.8368, AUC: 0.8318\n",
            "Epoch 4/10\n",
            "Train - Loss: 0.4353, Acc: 0.8273, F1: 0.8242, AUC: 0.8353\n",
            "Val   - Loss: 0.4217, Acc: 0.8389, F1: 0.8438, AUC: 0.8427\n",
            "Epoch 5/10\n",
            "Train - Loss: 0.4290, Acc: 0.8302, F1: 0.8275, AUC: 0.8381\n",
            "Val   - Loss: 0.4179, Acc: 0.8365, F1: 0.8396, AUC: 0.8413\n",
            "Epoch 6/10\n",
            "Train - Loss: 0.4213, Acc: 0.8335, F1: 0.8306, AUC: 0.8415\n",
            "Val   - Loss: 0.4169, Acc: 0.8409, F1: 0.8408, AUC: 0.8476\n",
            "Epoch 7/10\n",
            "Train - Loss: 0.4161, Acc: 0.8369, F1: 0.8339, AUC: 0.8450\n",
            "Val   - Loss: 0.4213, Acc: 0.8263, F1: 0.8379, AUC: 0.8264\n",
            "Epoch 8/10\n",
            "Train - Loss: 0.4110, Acc: 0.8424, F1: 0.8398, AUC: 0.8504\n",
            "Val   - Loss: 0.4087, Acc: 0.8530, F1: 0.8484, AUC: 0.8623\n",
            "Epoch 9/10\n",
            "Train - Loss: 0.4080, Acc: 0.8457, F1: 0.8433, AUC: 0.8536\n",
            "Val   - Loss: 0.3946, Acc: 0.8600, F1: 0.8570, AUC: 0.8685\n",
            "Epoch 10/10\n",
            "Train - Loss: 0.4025, Acc: 0.8511, F1: 0.8495, AUC: 0.8587\n",
            "Val   - Loss: 0.4031, Acc: 0.8474, F1: 0.8395, AUC: 0.8585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CRoss"
      ],
      "metadata": {
        "id": "hFoTtrV90oo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        sequences = sequences.to(device, non_blocking=True) if sequences.device != device else sequences\n",
        "        labels = labels.to(device, non_blocking=True) if labels.device != device else labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logprobs, _ = model(sequences)\n",
        "        loss = criterion(logprobs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "        preds = torch.argmax(logprobs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_labels, all_preds = [], []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            logprobs, _ = model(sequences)\n",
        "            loss = criterion(logprobs, labels)\n",
        "            running_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "            preds = torch.argmax(logprobs, dim=1)\n",
        "            probs = torch.softmax(logprobs, dim=1)[:, 1]\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    return avg_loss, accuracy, auc, f1"
      ],
      "metadata": {
        "id": "dOU_yfx70m6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def cross_validate_model(dataset, model, criterion, optimizer_fn, device, num_folds=5):\n",
        "\n",
        "    labels = [label.cpu().item() if isinstance(label, torch.Tensor) else label for _, label in dataset]\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "    fold_metrics = {\"accuracy\": [], \"auc\": [], \"f1\": []}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(dataset, labels)):\n",
        "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
        "\n",
        "        train_subset = Subset(dataset, train_idx)\n",
        "        val_subset = Subset(dataset, val_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "        model = model.to(device)\n",
        "        optimizer = optimizer_fn(model.parameters())\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(20):\n",
        "            train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # Validation\n",
        "        _, accuracy, auc, f1 = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Store metrics for each fold\n",
        "        fold_metrics[\"accuracy\"].append(accuracy.cpu().item())\n",
        "        fold_metrics[\"auc\"].append(auc.cpu().item())\n",
        "        fold_metrics[\"f1\"].append(f1.cpu().item())\n",
        "\n",
        "    # Calculate mean and std deviation for each metric\n",
        "    mean_metrics = {metric: np.mean(scores) for metric, scores in fold_metrics.items()}\n",
        "    std_metrics = {metric: np.std(scores) for metric, scores in fold_metrics.items()}\n",
        "\n",
        "    return mean_metrics, std_metrics\n"
      ],
      "metadata": {
        "id": "J-GJCRRV859N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "datasets = {\n",
        "    \"HumanEnhancersCohn\": (train_dset_1, test_dset_1),\n",
        "    # \"DemoCodingVsIntergenomicSeqs\": (train_dset_2, test_dset_2),\n",
        "    # \"DemoHumanOrWorm\": (train_dset_3, test_dset_3)\n",
        "}\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer_fn = lambda params: torch.optim.AdamW(params, lr=7e-5)\n",
        "\n",
        "for dataset_name, (train_dset, _) in datasets.items():\n",
        "    print(f\"\\nPerforming 5-fold cross-validation on {dataset_name}...\")\n",
        "\n",
        "    mean_metrics, std_metrics = cross_validate_model(\n",
        "        dataset=train_dset,\n",
        "        model=model,\n",
        "        criterion=criterion,\n",
        "        optimizer_fn=optimizer_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(f\"Accuracy: {mean_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
        "    print(f\"AUC: {mean_metrics['auc']:.4f} ± {std_metrics['auc']:.4f}\")\n",
        "    print(f\"F1 Score: {mean_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")"
      ],
      "metadata": {
        "id": "MfrDqhsR3T-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding"
      ],
      "metadata": {
        "id": "E2HHhsepGhzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DrosophilaEnhancersStark\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersEnsembl\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanOcrEnsembl"
      ],
      "metadata": {
        "id": "TYLF3b19BmdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dset = DrosophilaEnhancersStark('train', version=0)\n",
        "test_dset = DrosophilaEnhancersStark('test', version=0)"
      ],
      "metadata": {
        "id": "-LLGgIZpI-2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_sequences(dataset):\n",
        "    filtered_data = [(seq, label) for seq, label in dataset if 'N' not in seq]\n",
        "    return filtered_data\n",
        "\n",
        "def pad_sequences(sequences, target_length):\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        seq_len = len(seq)\n",
        "        if seq_len < target_length:\n",
        "\n",
        "            left_padding = (target_length - seq_len) // 2\n",
        "            right_padding = target_length - seq_len - left_padding\n",
        "            # Pad with zeros (for one-hot encoding later)\n",
        "            padded_seq = torch.nn.functional.pad(seq, (left_padding, right_padding))\n",
        "        else:\n",
        "            # Trim if necessary\n",
        "            padded_seq = seq[:target_length]\n",
        "        padded_sequences.append(padded_seq)\n",
        "    return torch.stack(padded_sequences)\n",
        "\n",
        "# # Custom collate function for DataLoader\n",
        "# def collate_fn(batch):\n",
        "#     sequences, labels = zip(*batch)\n",
        "#     sequence_lengths = [len(seq) for seq in sequences]\n",
        "\n",
        "#     # Calculate mean length for padding\n",
        "#     target_length = int(sum(sequence_lengths) / len(sequence_lengths))\n",
        "\n",
        "#     # Convert sequences and labels to tensors and apply padding\n",
        "#     padded_sequences = pad_sequences(sequences, target_length)\n",
        "#     labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "#     return padded_sequences, labels\n",
        "\n",
        "\n",
        "# Applying filtering to dataset\n",
        "filtered_train_data = filter_sequences(train_dset)\n",
        "filtered_test_data = filter_sequences(test_dset)"
      ],
      "metadata": {
        "id": "lw6uumTRP4N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sequence(sequence):\n",
        "    encoded_seq = [encoding_map[base] for base in sequence]\n",
        "    return torch.tensor(encoded_seq, dtype=torch.float).t()\n",
        "\n",
        "def preprocess_sequences(dataset):\n",
        "    preprocessed_data = []\n",
        "    for seq, label in dataset:\n",
        "        if 'N' not in seq:  # Filter out sequences with 'N'\n",
        "            encoded_seq = encode_sequence(seq)\n",
        "            preprocessed_data.append((encoded_seq, label))\n",
        "    return preprocessed_data\n",
        "\n",
        "def pad_sequences(sequences, target_length):\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        seq_len = seq.size(1)\n",
        "        if seq_len < target_length:\n",
        "\n",
        "            left_padding = (target_length - seq_len) // 2\n",
        "            right_padding = target_length - seq_len - left_padding\n",
        "\n",
        "            padded_seq = torch.nn.functional.pad(seq, (left_padding, right_padding))\n",
        "        else:\n",
        "\n",
        "            padded_seq = seq[:, :target_length]\n",
        "        padded_sequences.append(padded_seq)\n",
        "    return torch.stack(padded_sequences)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "    sequence_lengths = [seq.size(1) for seq in sequences]\n",
        "    target_length = int(np.median(sequence_lengths))\n",
        "    padded_sequences = pad_sequences(sequences, target_length)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return padded_sequences, labels"
      ],
      "metadata": {
        "id": "qNaTr_J-N02j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_train_data = preprocess_sequences(train_dset)\n",
        "filtered_test_data = preprocess_sequences(test_dset)\n",
        "\n",
        "seq_length = 500\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(filtered_train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "test_loader = DataLoader(filtered_test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
      ],
      "metadata": {
        "id": "AN2naYbWQs6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SeqNN(\n",
        "    seqsize=seq_length,\n",
        "    use_single_channel=False,\n",
        "    use_reverse_channel=False,\n",
        "    use_multisubstate_channel=False,\n",
        "    final_ch=1  # For binary classification\n",
        ")\n",
        "# criterion = nn.BCEWithLogitsLoss()  # Binary classification with logits\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.005)"
      ],
      "metadata": {
        "id": "LW7xCZmlP8p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "-QSjw8pjRAOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc, train_f1, train_auc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_f1, val_auc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}\")\n",
        "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")"
      ],
      "metadata": {
        "id": "q1hOkdG0QWEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bilinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Bilinear layer that uses a single Linear layer for pairwise interaction.\n",
        "    \"\"\"\n",
        "    def __init__(self, n: int, out=None, bias=False):\n",
        "        super().__init__()\n",
        "        if out is None:\n",
        "            out = n\n",
        "        self.fc = nn.Linear(n, out, bias=bias)\n",
        "        # self.fc = KANLinear(n, out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Squeeze-and-Excite layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    inp : int\n",
        "        Middle layer size.\n",
        "    oup : int\n",
        "        Input and output size.\n",
        "    reduction : int, optional\n",
        "        Reduction parameter. Default is 4.\n",
        "    \"\"\"\n",
        "    def __init__(self, inp, oup, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "\n",
        "            nn.Linear(oup, inp // reduction, bias=False),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Linear(inp // reduction, oup, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _ = x.size()\n",
        "        y = x.view(b, c, -1).mean(dim=2)  # Global Average Pooling\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y\n",
        "\n",
        "# Main SeqNN model\n",
        "class SeqNN(nn.Module):\n",
        "    \"\"\"\n",
        "    LegNet neural network for binary classification.\n",
        "    \"\"\"\n",
        "    __constants__ = ('resize_factor')\n",
        "\n",
        "    def __init__(self,\n",
        "                seqsize,\n",
        "                use_single_channel,\n",
        "                use_reverse_channel,\n",
        "                use_multisubstate_channel,\n",
        "                block_sizes=[256, 256, 128, 128, 64, 64, 32, 32],\n",
        "                ks=5,\n",
        "                resize_factor=4,\n",
        "                activation=nn.SiLU,\n",
        "                filter_per_group=2,\n",
        "                se_reduction=4,\n",
        "                final_ch=1,\n",
        "                bn_momentum=0.1):\n",
        "        super().__init__()\n",
        "        self.block_sizes = block_sizes\n",
        "        self.resize_factor = resize_factor\n",
        "        self.se_reduction = se_reduction\n",
        "        self.seqsize = seqsize\n",
        "        self.use_single_channel = use_single_channel\n",
        "        self.use_reverse_channel = use_reverse_channel\n",
        "        self.use_multisubstate_channel = use_multisubstate_channel\n",
        "        self.final_ch = final_ch\n",
        "        self.bn_momentum = bn_momentum\n",
        "        seqextblocks = OrderedDict()\n",
        "\n",
        "        in_channels_first_block = 4\n",
        "        if self.use_single_channel:\n",
        "            in_channels_first_block += 1\n",
        "        if self.use_reverse_channel:\n",
        "            in_channels_first_block += 1\n",
        "        if self.use_multisubstate_channel:\n",
        "            in_channels_first_block += 1\n",
        "\n",
        "        block = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=in_channels_first_block,\n",
        "                out_channels=block_sizes[0],\n",
        "                kernel_size=ks,\n",
        "                padding='same',\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm1d(block_sizes[0], momentum=self.bn_momentum),\n",
        "            activation()\n",
        "        )\n",
        "        seqextblocks['blc0'] = block\n",
        "\n",
        "        # Building remaining blocks\n",
        "        for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
        "            block = nn.Sequential(\n",
        "                nn.Conv1d(prev_sz, sz * self.resize_factor, kernel_size=1, padding='same', bias=False),\n",
        "                nn.BatchNorm1d(sz * self.resize_factor, momentum=self.bn_momentum),\n",
        "                activation(),\n",
        "\n",
        "                nn.Conv1d(sz * self.resize_factor, sz * self.resize_factor, kernel_size=ks,\n",
        "                          groups=sz * self.resize_factor // filter_per_group, padding='same', bias=False),\n",
        "                nn.BatchNorm1d(sz * self.resize_factor, momentum=self.bn_momentum),\n",
        "                activation(),\n",
        "\n",
        "                SELayer(prev_sz, sz * self.resize_factor, reduction=self.se_reduction),\n",
        "\n",
        "                nn.Conv1d(sz * self.resize_factor, prev_sz, kernel_size=1, padding='same', bias=False),\n",
        "                nn.BatchNorm1d(prev_sz, momentum=self.bn_momentum),\n",
        "                activation(),\n",
        "            )\n",
        "            seqextblocks[f'inv_res_blc{ind}'] = block\n",
        "\n",
        "            resize_block = nn.Sequential(\n",
        "                nn.Conv1d(2 * prev_sz, sz, kernel_size=ks, padding='same', bias=False),\n",
        "                nn.BatchNorm1d(sz, momentum=self.bn_momentum),\n",
        "                activation()\n",
        "            )\n",
        "            seqextblocks[f'resize_blc{ind}'] = resize_block\n",
        "\n",
        "        self.seqextractor = nn.ModuleDict(seqextblocks)\n",
        "\n",
        "        self.mapper = nn.Sequential(\n",
        "            nn.Conv1d(block_sizes[-1], self.final_ch, kernel_size=1, padding='same'),\n",
        "            activation()\n",
        "        )\n",
        "\n",
        "        self.register_buffer('bins', torch.arange(start=0, end=self.final_ch, step=1, requires_grad=False))\n",
        "\n",
        "    def feature_extractor(self, x):\n",
        "        x = self.seqextractor['blc0'](x)\n",
        "        for i in range(len(self.block_sizes) - 1):\n",
        "            x = torch.cat([x, self.seqextractor[f'inv_res_blc{i}'](x)], dim=1)\n",
        "            x = self.seqextractor[f'resize_blc{i}'](x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.feature_extractor(x)\n",
        "        x = self.mapper(f)\n",
        "        x = F.adaptive_avg_pool1d(x, 1)\n",
        "        x = x.squeeze(2)\n",
        "        prob = torch.sigmoid(x).squeeze(1)\n",
        "\n",
        "        return prob\n",
        "        # logprobs = F.log_softmax(x, dim=1)\n",
        "\n",
        "        # # Soft-argmax operation (optional)\n",
        "        # x = F.softmax(x, dim=1)\n",
        "        # score = (x * self.bins).sum(dim=1)\n",
        "\n",
        "        # return logprobs, score"
      ],
      "metadata": {
        "id": "T0vq3LoQtvQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SeqNN(\n",
        "    seqsize=512,\n",
        "    use_single_channel=False,\n",
        "    use_reverse_channel=False,\n",
        "    use_multisubstate_channel=False,\n",
        "    final_ch=1\n",
        ")\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.005)"
      ],
      "metadata": {
        "id": "dNOb0ZHEt7RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6Hd83MAtvn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore"
      ],
      "metadata": {
        "id": "7kZEPVm0tfwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fvcore.nn import FlopCountAnalysis\n"
      ],
      "metadata": {
        "id": "SYnEewxYgezS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FLOPs: \", flops.total())\n",
        "print(\"FLOPs per layer: \", flops.by_module())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62yfmMtCtgxK",
        "outputId": "805ad608-bab6-4023-a959-b9e76457b1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::_convolution_mode encountered 30 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 29 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::silu encountered 44 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 7 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::lt encountered 14 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 168 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 84 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 105 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 56 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sigmoid encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::adaptive_avg_pool1d encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "seqextractor.inv_res_blc0.6.avg_pool, seqextractor.inv_res_blc1.6.avg_pool, seqextractor.inv_res_blc2.6.avg_pool, seqextractor.inv_res_blc3.6.avg_pool, seqextractor.inv_res_blc4.6.avg_pool, seqextractor.inv_res_blc5.6.avg_pool, seqextractor.inv_res_blc6.6.avg_pool\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs:  21592064\n",
            "FLOPs per layer:  Counter({'': 21592064, 'seqextractor': 21592064, 'seqextractor.inv_res_blc0': 7077888, 'seqextractor.inv_res_blc1': 3866624, 'seqextractor.inv_res_blc2': 3244032, 'seqextractor.inv_res_blc0.1': 2621440, 'seqextractor.inv_res_blc0.4': 2621440, 'seqextractor.inv_res_blc3': 1785856, 'seqextractor.inv_res_blc4': 1548288, 'seqextractor.inv_res_blc1.1': 1310720, 'seqextractor.inv_res_blc1.4': 1310720, 'seqextractor.inv_res_blc2.1': 1310720, 'seqextractor.inv_res_blc2.4': 1310720, 'seqextractor.inv_res_blc0.6': 1179648, 'seqextractor.inv_res_blc0.6.fc': 1179648, 'seqextractor.inv_res_blc5': 856064, 'seqextractor.inv_res_blc6': 755712, 'seqextractor.blc0': 655360, 'seqextractor.blc0.1': 655360, 'seqextractor.inv_res_blc0.8': 655360, 'seqextractor.resize_blc0': 655360, 'seqextractor.resize_blc0.1': 655360, 'seqextractor.inv_res_blc1.8': 655360, 'seqextractor.inv_res_blc3.1': 655360, 'seqextractor.inv_res_blc3.4': 655360, 'seqextractor.inv_res_blc4.1': 655360, 'seqextractor.inv_res_blc4.4': 655360, 'seqextractor.inv_res_blc0.6.fc.0': 589824, 'seqextractor.inv_res_blc0.6.fc.1': 589824, 'seqextractor.inv_res_blc1.6': 589824, 'seqextractor.inv_res_blc1.6.fc': 589824, 'seqextractor.resize_blc1': 327680, 'seqextractor.resize_blc1.1': 327680, 'seqextractor.inv_res_blc2.8': 327680, 'seqextractor.resize_blc2': 327680, 'seqextractor.resize_blc2.1': 327680, 'seqextractor.inv_res_blc3.8': 327680, 'seqextractor.inv_res_blc5.1': 327680, 'seqextractor.inv_res_blc5.4': 327680, 'seqextractor.inv_res_blc6.1': 327680, 'seqextractor.inv_res_blc6.4': 327680, 'seqextractor.inv_res_blc1.6.fc.0': 294912, 'seqextractor.inv_res_blc1.6.fc.1': 294912, 'seqextractor.inv_res_blc2.6': 294912, 'seqextractor.inv_res_blc2.6.fc': 294912, 'seqextractor.resize_blc3': 163840, 'seqextractor.resize_blc3.1': 163840, 'seqextractor.inv_res_blc4.8': 163840, 'seqextractor.resize_blc4': 163840, 'seqextractor.resize_blc4.1': 163840, 'seqextractor.inv_res_blc5.8': 163840, 'seqextractor.inv_res_blc2.6.fc.0': 147456, 'seqextractor.inv_res_blc2.6.fc.1': 147456, 'seqextractor.inv_res_blc3.6': 147456, 'seqextractor.inv_res_blc3.6.fc': 147456, 'seqextractor.resize_blc5': 81920, 'seqextractor.resize_blc5.1': 81920, 'seqextractor.inv_res_blc6.8': 81920, 'seqextractor.resize_blc6': 81920, 'seqextractor.resize_blc6.1': 81920, 'seqextractor.inv_res_blc3.6.fc.0': 73728, 'seqextractor.inv_res_blc3.6.fc.1': 73728, 'seqextractor.inv_res_blc4.6': 73728, 'seqextractor.inv_res_blc4.6.fc': 73728, 'seqextractor.inv_res_blc4.6.fc.0': 36864, 'seqextractor.inv_res_blc4.6.fc.1': 36864, 'seqextractor.inv_res_blc5.6': 36864, 'seqextractor.inv_res_blc5.6.fc': 36864, 'seqextractor.inv_res_blc5.6.fc.0': 18432, 'seqextractor.inv_res_blc5.6.fc.1': 18432, 'seqextractor.inv_res_blc6.6': 18432, 'seqextractor.inv_res_blc6.6.fc': 18432, 'seqextractor.inv_res_blc6.6.fc.0': 9216, 'seqextractor.inv_res_blc6.6.fc.1': 9216, 'seqextractor.blc0.0': 0, 'seqextractor.blc0.2': 0, 'seqextractor.inv_res_blc0.0': 0, 'seqextractor.inv_res_blc0.2': 0, 'seqextractor.inv_res_blc0.3': 0, 'seqextractor.inv_res_blc0.5': 0, 'seqextractor.inv_res_blc0.6.avg_pool': 0, 'seqextractor.inv_res_blc0.6.fc.0.base_activation': 0, 'seqextractor.inv_res_blc0.6.fc.1.base_activation': 0, 'seqextractor.inv_res_blc0.6.fc.2': 0, 'seqextractor.inv_res_blc0.7': 0, 'seqextractor.inv_res_blc0.9': 0, 'seqextractor.resize_blc0.0': 0, 'seqextractor.resize_blc0.2': 0, 'seqextractor.inv_res_blc1.0': 0, 'seqextractor.inv_res_blc1.2': 0, 'seqextractor.inv_res_blc1.3': 0, 'seqextractor.inv_res_blc1.5': 0, 'seqextractor.inv_res_blc1.6.avg_pool': 0, 'seqextractor.inv_res_blc1.6.fc.0.base_activation': 0, 'seqextractor.inv_res_blc1.6.fc.1.base_activation': 0, 'seqextractor.inv_res_blc1.6.fc.2': 0, 'seqextractor.inv_res_blc1.7': 0, 'seqextractor.inv_res_blc1.9': 0, 'seqextractor.resize_blc1.0': 0, 'seqextractor.resize_blc1.2': 0, 'seqextractor.inv_res_blc2.0': 0, 'seqextractor.inv_res_blc2.2': 0, 'seqextractor.inv_res_blc2.3': 0, 'seqextractor.inv_res_blc2.5': 0, 'seqextractor.inv_res_blc2.6.avg_pool': 0, 'seqextractor.inv_res_blc2.6.fc.0.base_activation': 0, 'seqextractor.inv_res_blc2.6.fc.1.base_activation': 0, 'seqextractor.inv_res_blc2.6.fc.2': 0, 'seqextractor.inv_res_blc2.7': 0, 'seqextractor.inv_res_blc2.9': 0, 'seqextractor.resize_blc2.0': 0, 'seqextractor.resize_blc2.2': 0, 'seqextractor.inv_res_blc3.0': 0, 'seqextractor.inv_res_blc3.2': 0, 'seqextractor.inv_res_blc3.3': 0, 'seqextractor.inv_res_blc3.5': 0, 'seqextractor.inv_res_blc3.6.avg_pool': 0, 'seqextractor.inv_res_blc3.6.fc.0.base_activation': 0, 'seqextractor.inv_res_blc3.6.fc.1.base_activation': 0, 'seqextractor.inv_res_blc3.6.fc.2': 0, 'seqextractor.inv_res_blc3.7': 0, 'seqextractor.inv_res_blc3.9': 0, 'seqextractor.resize_blc3.0': 0, 'seqextractor.resize_blc3.2': 0, 'seqextractor.inv_res_blc4.0': 0, 'seqextractor.inv_res_blc4.2': 0, 'seqextractor.inv_res_blc4.3': 0, 'seqextractor.inv_res_blc4.5': 0, 'seqextractor.inv_res_blc4.6.avg_pool': 0, 'seqextractor.inv_res_blc4.6.fc.0.base_activation': 0, 'seqextractor.inv_res_blc4.6.fc.1.base_activation': 0, 'seqextractor.inv_res_blc4.6.fc.2': 0, 'seqextractor.inv_res_blc4.7': 0, 'seqextractor.inv_res_blc4.9': 0, 'seqextractor.resize_blc4.0': 0, 'seqextractor.resize_blc4.2': 0, 'seqextractor.inv_res_blc5.0': 0, 'seqextractor.inv_res_blc5.2': 0, 'seqextractor.inv_res_blc5.3': 0, 'seqextractor.inv_res_blc5.5': 0, 'seqextractor.inv_res_blc5.6.avg_pool': 0, 'seqextractor.inv_res_blc5.6.fc.0.base_activation': 0, 'seqextractor.inv_res_blc5.6.fc.1.base_activation': 0, 'seqextractor.inv_res_blc5.6.fc.2': 0, 'seqextractor.inv_res_blc5.7': 0, 'seqextractor.inv_res_blc5.9': 0, 'seqextractor.resize_blc5.0': 0, 'seqextractor.resize_blc5.2': 0, 'seqextractor.inv_res_blc6.0': 0, 'seqextractor.inv_res_blc6.2': 0, 'seqextractor.inv_res_blc6.3': 0, 'seqextractor.inv_res_blc6.5': 0, 'seqextractor.inv_res_blc6.6.avg_pool': 0, 'seqextractor.inv_res_blc6.6.fc.0.base_activation': 0, 'seqextractor.inv_res_blc6.6.fc.1.base_activation': 0, 'seqextractor.inv_res_blc6.6.fc.2': 0, 'seqextractor.inv_res_blc6.7': 0, 'seqextractor.inv_res_blc6.9': 0, 'seqextractor.resize_blc6.0': 0, 'seqextractor.resize_blc6.2': 0, 'mapper': 0, 'mapper.0': 0, 'mapper.1': 0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flops = FlopCountAnalysis(model, input)\n",
        "print(\"FLOPs: \", flops.total())\n",
        "print(\"FLOPs per layer: \", flops.by_module())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdPmFmIftt6U",
        "outputId": "9b34230e-6d30-4a6d-f183-9c4cdb074ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::_convolution_mode encountered 30 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 29 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::silu encountered 37 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 7 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sigmoid encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 7 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::adaptive_avg_pool1d encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "seqextractor.inv_res_blc0.6.avg_pool, seqextractor.inv_res_blc1.6.avg_pool, seqextractor.inv_res_blc2.6.avg_pool, seqextractor.inv_res_blc3.6.avg_pool, seqextractor.inv_res_blc4.6.avg_pool, seqextractor.inv_res_blc5.6.avg_pool, seqextractor.inv_res_blc6.6.avg_pool\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs:  19511296\n",
            "FLOPs per layer:  Counter({'': 19511296, 'seqextractor': 19511296, 'seqextractor.inv_res_blc0': 6029312, 'seqextractor.inv_res_blc1': 3342336, 'seqextractor.inv_res_blc2': 2981888, 'seqextractor.inv_res_blc0.1': 2621440, 'seqextractor.inv_res_blc0.4': 2621440, 'seqextractor.inv_res_blc3': 1654784, 'seqextractor.inv_res_blc4': 1482752, 'seqextractor.inv_res_blc1.1': 1310720, 'seqextractor.inv_res_blc1.4': 1310720, 'seqextractor.inv_res_blc2.1': 1310720, 'seqextractor.inv_res_blc2.4': 1310720, 'seqextractor.inv_res_blc5': 823296, 'seqextractor.inv_res_blc6': 739328, 'seqextractor.blc0': 655360, 'seqextractor.blc0.1': 655360, 'seqextractor.inv_res_blc0.8': 655360, 'seqextractor.resize_blc0': 655360, 'seqextractor.resize_blc0.1': 655360, 'seqextractor.inv_res_blc1.8': 655360, 'seqextractor.inv_res_blc3.1': 655360, 'seqextractor.inv_res_blc3.4': 655360, 'seqextractor.inv_res_blc4.1': 655360, 'seqextractor.inv_res_blc4.4': 655360, 'seqextractor.resize_blc1': 327680, 'seqextractor.resize_blc1.1': 327680, 'seqextractor.inv_res_blc2.8': 327680, 'seqextractor.resize_blc2': 327680, 'seqextractor.resize_blc2.1': 327680, 'seqextractor.inv_res_blc3.8': 327680, 'seqextractor.inv_res_blc5.1': 327680, 'seqextractor.inv_res_blc5.4': 327680, 'seqextractor.inv_res_blc6.1': 327680, 'seqextractor.inv_res_blc6.4': 327680, 'seqextractor.resize_blc3': 163840, 'seqextractor.resize_blc3.1': 163840, 'seqextractor.inv_res_blc4.8': 163840, 'seqextractor.resize_blc4': 163840, 'seqextractor.resize_blc4.1': 163840, 'seqextractor.inv_res_blc5.8': 163840, 'seqextractor.inv_res_blc0.6': 131072, 'seqextractor.inv_res_blc0.6.fc': 131072, 'seqextractor.resize_blc5': 81920, 'seqextractor.resize_blc5.1': 81920, 'seqextractor.inv_res_blc6.8': 81920, 'seqextractor.resize_blc6': 81920, 'seqextractor.resize_blc6.1': 81920, 'seqextractor.inv_res_blc0.6.fc.0': 65536, 'seqextractor.inv_res_blc0.6.fc.2': 65536, 'seqextractor.inv_res_blc1.6': 65536, 'seqextractor.inv_res_blc1.6.fc': 65536, 'seqextractor.inv_res_blc1.6.fc.0': 32768, 'seqextractor.inv_res_blc1.6.fc.2': 32768, 'seqextractor.inv_res_blc2.6': 32768, 'seqextractor.inv_res_blc2.6.fc': 32768, 'seqextractor.inv_res_blc2.6.fc.0': 16384, 'seqextractor.inv_res_blc2.6.fc.2': 16384, 'seqextractor.inv_res_blc3.6': 16384, 'seqextractor.inv_res_blc3.6.fc': 16384, 'seqextractor.inv_res_blc3.6.fc.0': 8192, 'seqextractor.inv_res_blc3.6.fc.2': 8192, 'seqextractor.inv_res_blc4.6': 8192, 'seqextractor.inv_res_blc4.6.fc': 8192, 'seqextractor.inv_res_blc4.6.fc.0': 4096, 'seqextractor.inv_res_blc4.6.fc.2': 4096, 'seqextractor.inv_res_blc5.6': 4096, 'seqextractor.inv_res_blc5.6.fc': 4096, 'seqextractor.inv_res_blc5.6.fc.0': 2048, 'seqextractor.inv_res_blc5.6.fc.2': 2048, 'seqextractor.inv_res_blc6.6': 2048, 'seqextractor.inv_res_blc6.6.fc': 2048, 'seqextractor.inv_res_blc6.6.fc.0': 1024, 'seqextractor.inv_res_blc6.6.fc.2': 1024, 'seqextractor.blc0.0': 0, 'seqextractor.blc0.2': 0, 'seqextractor.inv_res_blc0.0': 0, 'seqextractor.inv_res_blc0.2': 0, 'seqextractor.inv_res_blc0.3': 0, 'seqextractor.inv_res_blc0.5': 0, 'seqextractor.inv_res_blc0.6.avg_pool': 0, 'seqextractor.inv_res_blc0.6.fc.1': 0, 'seqextractor.inv_res_blc0.6.fc.3': 0, 'seqextractor.inv_res_blc0.7': 0, 'seqextractor.inv_res_blc0.9': 0, 'seqextractor.resize_blc0.0': 0, 'seqextractor.resize_blc0.2': 0, 'seqextractor.inv_res_blc1.0': 0, 'seqextractor.inv_res_blc1.2': 0, 'seqextractor.inv_res_blc1.3': 0, 'seqextractor.inv_res_blc1.5': 0, 'seqextractor.inv_res_blc1.6.avg_pool': 0, 'seqextractor.inv_res_blc1.6.fc.1': 0, 'seqextractor.inv_res_blc1.6.fc.3': 0, 'seqextractor.inv_res_blc1.7': 0, 'seqextractor.inv_res_blc1.9': 0, 'seqextractor.resize_blc1.0': 0, 'seqextractor.resize_blc1.2': 0, 'seqextractor.inv_res_blc2.0': 0, 'seqextractor.inv_res_blc2.2': 0, 'seqextractor.inv_res_blc2.3': 0, 'seqextractor.inv_res_blc2.5': 0, 'seqextractor.inv_res_blc2.6.avg_pool': 0, 'seqextractor.inv_res_blc2.6.fc.1': 0, 'seqextractor.inv_res_blc2.6.fc.3': 0, 'seqextractor.inv_res_blc2.7': 0, 'seqextractor.inv_res_blc2.9': 0, 'seqextractor.resize_blc2.0': 0, 'seqextractor.resize_blc2.2': 0, 'seqextractor.inv_res_blc3.0': 0, 'seqextractor.inv_res_blc3.2': 0, 'seqextractor.inv_res_blc3.3': 0, 'seqextractor.inv_res_blc3.5': 0, 'seqextractor.inv_res_blc3.6.avg_pool': 0, 'seqextractor.inv_res_blc3.6.fc.1': 0, 'seqextractor.inv_res_blc3.6.fc.3': 0, 'seqextractor.inv_res_blc3.7': 0, 'seqextractor.inv_res_blc3.9': 0, 'seqextractor.resize_blc3.0': 0, 'seqextractor.resize_blc3.2': 0, 'seqextractor.inv_res_blc4.0': 0, 'seqextractor.inv_res_blc4.2': 0, 'seqextractor.inv_res_blc4.3': 0, 'seqextractor.inv_res_blc4.5': 0, 'seqextractor.inv_res_blc4.6.avg_pool': 0, 'seqextractor.inv_res_blc4.6.fc.1': 0, 'seqextractor.inv_res_blc4.6.fc.3': 0, 'seqextractor.inv_res_blc4.7': 0, 'seqextractor.inv_res_blc4.9': 0, 'seqextractor.resize_blc4.0': 0, 'seqextractor.resize_blc4.2': 0, 'seqextractor.inv_res_blc5.0': 0, 'seqextractor.inv_res_blc5.2': 0, 'seqextractor.inv_res_blc5.3': 0, 'seqextractor.inv_res_blc5.5': 0, 'seqextractor.inv_res_blc5.6.avg_pool': 0, 'seqextractor.inv_res_blc5.6.fc.1': 0, 'seqextractor.inv_res_blc5.6.fc.3': 0, 'seqextractor.inv_res_blc5.7': 0, 'seqextractor.inv_res_blc5.9': 0, 'seqextractor.resize_blc5.0': 0, 'seqextractor.resize_blc5.2': 0, 'seqextractor.inv_res_blc6.0': 0, 'seqextractor.inv_res_blc6.2': 0, 'seqextractor.inv_res_blc6.3': 0, 'seqextractor.inv_res_blc6.5': 0, 'seqextractor.inv_res_blc6.6.avg_pool': 0, 'seqextractor.inv_res_blc6.6.fc.1': 0, 'seqextractor.inv_res_blc6.6.fc.3': 0, 'seqextractor.inv_res_blc6.7': 0, 'seqextractor.inv_res_blc6.9': 0, 'seqextractor.resize_blc6.0': 0, 'seqextractor.resize_blc6.2': 0, 'mapper': 0, 'mapper.0': 0, 'mapper.1': 0})\n"
          ]
        }
      ]
    }
  ]
}